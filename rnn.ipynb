{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Model to Determine the nationality of a given 'name'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Download the dataset\n",
    "#!curl -O https://download.pytorch.org/tutorial/data.zip; \n",
    "\n",
    "#! unzip data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependancies\n",
    "import os\n",
    "import random\n",
    "from string import ascii_letters\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from unidecode import unidecode\n",
    "\n",
    "\n",
    "torch.manual_seed(2)\n",
    "\n",
    "data_dir = \"data/names\"\n",
    "arabic_file = os.path.join(data_dir, 'Arabic.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Languages (classes): 18\n"
     ]
    }
   ],
   "source": [
    "all_files = os.listdir(data_dir)\n",
    "num_langs = len(all_files)\n",
    "lang2label  = {file_name.split('.')[0]: torch.tensor([i], dtype = torch.long) for i, file_name in enumerate(all_files)}\n",
    "\n",
    "#Output: {'Czech': tensor(0),\n",
    "         #'German': tensor(1),...}\n",
    "print(f\"Total Languages (classes): {num_langs}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters in the Vocab: 59\n"
     ]
    }
   ],
   "source": [
    "vocab = dict()\n",
    "for i, letter in enumerate(ascii_letters + \" .,:;-'\"):\n",
    "    vocab.update({letter:i})\n",
    "\n",
    "# Vocab\n",
    "vocab_size = len(vocab) \n",
    "\n",
    "print(f'Total Characters in the Vocab: {vocab_size}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are a total of 59 tokens in our character vocabulary. This includes spaces and punctuations, such as ` .,:;-â€˜. <br>\n",
    "This also means that each name will now be expressed as a tensor of size (num_char, 59). <br>\n",
    "One-hot vector for each character. so if 5 characters in a name. 5 one-hot vectors."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to create one-hot vectors for name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def name2tensor(name,vocab):\n",
    "    '''\n",
    "    Converts a name to a tensor of size (len(name), len(vocab))\n",
    "    '''\n",
    "    base_tensor = torch.zeros(len(name),1,  len(vocab))\n",
    "    #*the extra dimension in the above tensor is bcos pytorch expects everything in a batch.\n",
    "    for i, chars in enumerate(name):\n",
    "        idx = vocab[chars]\n",
    "        base_tensor[i][0][idx] = 1 \n",
    "        \n",
    "    return base_tensor\n",
    "        \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key Not Present\n",
      "Maxa/B\n",
      "Key Not Present\n",
      "Rafaj1\n",
      "Key Not Present\n",
      "Urbanek1\n",
      "Key Not Present\n",
      "Whitmire1\n",
      "Total Names in all files: 20074\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "def create_dataset(data_dir, all_files):\n",
    "    names = 0\n",
    "    c = 0\n",
    "    tensor_names= []\n",
    "    tensor_labels = []\n",
    "    for file in all_files:\n",
    "        with open(os.path.join(data_dir, file)) as f:\n",
    "            lang = file.split('.')[0]\n",
    "            names = [unidecode(name.rstrip()) for name in f]\n",
    "            for name in names:\n",
    "                c += 1\n",
    "                try:\n",
    "                    tensor_names.append(name2tensor(name, vocab)) # This is a one-hot vector for every character\n",
    "                    tensor_labels.append(lang2label[lang])  #These are integer labels\n",
    "                except KeyError:\n",
    "                    print('Key Not Present')\n",
    "                    print(name)\n",
    "                    pass\n",
    "    print(f'Total Names in all files: {c}')\n",
    "    return tensor_names, tensor_labels\n",
    "            \n",
    "tensor_names, tensor_labels = create_dataset(data_dir, all_files)\n",
    "print(\"Done\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Training Examples: 18063\n",
      "Total Testing Examples: 2007\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(tensor_names, tensor_labels, test_size = 0.1, shuffle= True, stratify = tensor_labels)\n",
    "\n",
    "train_set = list(zip(xtrain, ytrain))\n",
    "test_set = list(zip(xtest, ytest))\n",
    "\n",
    "print(f\"Total Training Examples: {len(train_set)}\")\n",
    "print(f\"Total Testing Examples: {len(test_set)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_rnn import Classification_RNN\n",
    "\n",
    "# Initialize hyperparameters and modules\n",
    "hidden_size = 256\n",
    "learning_rate = 1e-3\n",
    "model = Classification_RNN(vocab_size, hidden_size, num_langs)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/3], Step [3000/18063], Loss: 0.1314\n",
      "Epoch: [1/3], Step [6000/18063], Loss: 4.7553\n",
      "Epoch: [1/3], Step [9000/18063], Loss: 0.0037\n",
      "Epoch: [1/3], Step [12000/18063], Loss: 0.0015\n",
      "Epoch: [1/3], Step [15000/18063], Loss: 0.0586\n",
      "Epoch: [1/3], Step [18000/18063], Loss: 0.0000\n",
      "Epoch: [2/3], Step [3000/18063], Loss: 0.0112\n",
      "Epoch: [2/3], Step [6000/18063], Loss: 5.9061\n",
      "Epoch: [2/3], Step [9000/18063], Loss: 0.0000\n",
      "Epoch: [2/3], Step [12000/18063], Loss: 0.0000\n",
      "Epoch: [2/3], Step [15000/18063], Loss: 0.0293\n",
      "Epoch: [2/3], Step [18000/18063], Loss: 0.0000\n",
      "Epoch: [3/3], Step [3000/18063], Loss: 0.0068\n",
      "Epoch: [3/3], Step [6000/18063], Loss: 5.8815\n",
      "Epoch: [3/3], Step [9000/18063], Loss: 0.0000\n",
      "Epoch: [3/3], Step [12000/18063], Loss: 0.0000\n",
      "Epoch: [3/3], Step [15000/18063], Loss: 0.0142\n",
      "Epoch: [3/3], Step [18000/18063], Loss: 0.0000\n",
      "Accuracy: 73.8416%\n"
     ]
    }
   ],
   "source": [
    "c = 0\n",
    "num_epochs = 3\n",
    "print_interval = 3000\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (name, label) in enumerate(train_set):\n",
    "        c += 1\n",
    "        hidden_state = model.init_hidden()\n",
    "        for char in name: #Looping through one-hot vectors\n",
    "            output, hidden_state = model(char, hidden_state)\n",
    " \n",
    "        loss = criterion(output, label) # output: 18-dim tensor , label: 1-dim tensor\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % print_interval == 0:\n",
    "            print(f\"Epoch: [{epoch+1}/{num_epochs}], \"\n",
    "                  f\"Step [{i + 1}/{len(train_set)}], \"\n",
    "                  f\"Loss: {loss.item():.4f}\"\n",
    "            )\n",
    "            \n",
    "num_correct = 0\n",
    "num_samples = len(test_set)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for name, label in test_set:\n",
    "        hidden_state = model.init_hidden()\n",
    "        for char in name:\n",
    "            output, hidden_state = model(char, hidden_state)\n",
    "        _, pred = torch.max(output, dim=1)\n",
    "        num_correct += bool(pred == label)\n",
    "\n",
    "print(f\"Accuracy: {num_correct / num_samples * 100:.4f}%\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: tensor([[ -5.2481,  -2.1307,  -7.1686,  -1.2919,  -2.9700,  -6.0612,  -3.1607,\n",
      "          -3.5493,  -4.0597,  -0.3481,  -5.9803,  -9.0225,  -6.2125,  -8.1270,\n",
      "          -5.1937,  -3.6715,  -4.4746, -10.1209]])\n",
      "a: tensor([-0.3481])\n",
      "pred: tensor([9])\n",
      "Ethnicity of Mike is 'English'\n"
     ]
    }
   ],
   "source": [
    "label2lang = {label.item():lang for lang, label in lang2label.items()}\n",
    "def predict(name):\n",
    "    model.eval()\n",
    "    name_tensor = name2tensor(name, vocab)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        hidden_state = model.init_hidden()\n",
    "        for char in name_tensor:\n",
    "            output, hidden_state = model(char, hidden_state)\n",
    "        print(f'output: {output}')\n",
    "        a, pred = torch.max(output, 1)\n",
    "        print(f\"a: {a}\")    \n",
    "        print(f\"pred: {pred}\")\n",
    "        \n",
    "    model.train()\n",
    "    \n",
    "    return label2lang[pred.item()]\n",
    "\n",
    "\n",
    "name = 'Mike'\n",
    "print(f\"Ethnicity of {name} is '{predict(name)}'\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model state_dict\n",
    "torch.save(model.state_dict(), 'models/classification_rnn.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8 (v3.8.8:024d8058b0, Feb 19 2021, 08:48:17) \n[Clang 6.0 (clang-600.0.57)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "082e9a3bcad0a290d0001e938aa60b99250c6c2ef33a923c00b70f9826caf4b7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
