{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Model to Determine the nationality of a given 'name'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Downloads the ready-dataset\n",
    "#!curl -O https://download.pytorch.org/tutorial/data.zip; \n",
    "#! unzip data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependancies\n",
    "import os\n",
    "import random\n",
    "from string import ascii_letters\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from unidecode import unidecode\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from utils import *\n",
    "from model_rnn import Classification_RNN\n",
    "\n",
    "torch.manual_seed(2)\n",
    "\n",
    "data_dir = \"data/names\"\n",
    "arabic_file = os.path.join(data_dir, 'Arabic.txt')\n",
    "all_files = os.listdir(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Vocab Size: 59\n",
      "Total Languages (classes): 18\n",
      "TOtal names in all languages: 20074\n",
      "Key Not Present\n",
      "Maxa/B\n",
      "Key Not Present\n",
      "Rafaj1\n",
      "Key Not Present\n",
      "Urbanek1\n",
      "Key Not Present\n",
      "Whitmire1\n",
      "Total Names in all files: 20074\n",
      "3\n",
      "Total Training Examples: 18063\n",
      "Total Testing Examples: 2007\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "#Create Vocab\n",
    "vocab, vocab_size = get_vocab(ascii_letters)\n",
    "\n",
    "#Create language-label dict\n",
    "lang2label, num_langs = get_lang2label(data_dir)\n",
    "\n",
    "#Create Dataset\n",
    "actual_names, tensor_names, tensor_labels = create_dataset(data_dir, all_files, vocab, lang2label)\n",
    "\n",
    "#Split into train and test\n",
    "train_names, xtrain,ytrain, test_names,xtest, ytest = split_train_test(actual_names, tensor_names, tensor_labels, test_size=0.10)\n",
    "train_set = list(zip(xtrain, ytrain))\n",
    "test_set = list(zip(xtest, ytest))\n",
    "\n",
    "print(f\"Total Training Examples: {len(train_set)}\")\n",
    "print(f\"Total Testing Examples: {len(test_set)}\")\n",
    "print(\"Done\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are a total of 59 tokens in our character vocabulary. This includes spaces and punctuations, such as ` .,:;-â€˜. <br>\n",
    "This also means that each name will now be expressed as a tensor of size (num_char, 59). <br>\n",
    "One-hot vector for each character. so if 5 characters in a name. 5 one-hot vectors. therefore tensor shape is -> (5,59) or (5, 1, 59) with a batch dimensions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize hyperparameters and modules\n",
    "hidden_size = 256\n",
    "learning_rate = 1e-3\n",
    "model = Classification_RNN(vocab_size, hidden_size, num_langs)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/5], Step [3000/18063], Loss: 0.0092\n",
      "Epoch: [1/5], Step [6000/18063], Loss: 1.9305\n",
      "Epoch: [1/5], Step [9000/18063], Loss: 4.3423\n",
      "Epoch: [1/5], Step [12000/18063], Loss: 0.3467\n",
      "Epoch: [1/5], Step [15000/18063], Loss: 0.0875\n",
      "Epoch: [1/5], Step [18000/18063], Loss: 0.0004\n",
      "Epoch: [2/5], Step [3000/18063], Loss: 0.0000\n",
      "Epoch: [2/5], Step [6000/18063], Loss: 0.7335\n",
      "Epoch: [2/5], Step [9000/18063], Loss: 4.6068\n",
      "Epoch: [2/5], Step [12000/18063], Loss: 0.1576\n",
      "Epoch: [2/5], Step [15000/18063], Loss: 0.0484\n",
      "Epoch: [2/5], Step [18000/18063], Loss: 0.0000\n",
      "Epoch: [3/5], Step [3000/18063], Loss: 0.0000\n",
      "Epoch: [3/5], Step [6000/18063], Loss: 0.7671\n",
      "Epoch: [3/5], Step [9000/18063], Loss: 4.0403\n",
      "Epoch: [3/5], Step [12000/18063], Loss: 0.1258\n",
      "Epoch: [3/5], Step [15000/18063], Loss: 0.0193\n",
      "Epoch: [3/5], Step [18000/18063], Loss: 0.0000\n",
      "Epoch: [4/5], Step [3000/18063], Loss: 0.0000\n",
      "Epoch: [4/5], Step [6000/18063], Loss: 1.2322\n",
      "Epoch: [4/5], Step [9000/18063], Loss: 3.8525\n",
      "Epoch: [4/5], Step [12000/18063], Loss: 0.0707\n",
      "Epoch: [4/5], Step [15000/18063], Loss: 0.0191\n",
      "Epoch: [4/5], Step [18000/18063], Loss: 0.0000\n",
      "Epoch: [5/5], Step [3000/18063], Loss: 0.0000\n",
      "Epoch: [5/5], Step [6000/18063], Loss: 0.7850\n",
      "Epoch: [5/5], Step [9000/18063], Loss: 3.7722\n",
      "Epoch: [5/5], Step [12000/18063], Loss: 0.1021\n",
      "Epoch: [5/5], Step [15000/18063], Loss: 0.0079\n",
      "Epoch: [5/5], Step [18000/18063], Loss: 0.0000\n",
      "Accuracy: 78.0269%\n"
     ]
    }
   ],
   "source": [
    "c = 0\n",
    "num_epochs = 5\n",
    "print_interval = 3000\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    for i, (name, label) in enumerate(train_set):\n",
    "        c += 1\n",
    "        hidden_state = model.init_hidden()\n",
    "        for char in name: #Looping through one-hot vectors of 59 each\n",
    "            output, hidden_state = model(char, hidden_state)\n",
    " \n",
    "        loss = criterion(output, label) # output: 18-dim tensor , label: 1-dim tensor\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % print_interval == 0:\n",
    "            print(f\"Epoch: [{epoch+1}/{num_epochs}], \"\n",
    "                  f\"Step [{i + 1}/{len(train_set)}], \"\n",
    "                  f\"Loss: {loss.item():.4f}\"\n",
    "            )\n",
    "            \n",
    "num_correct = 0\n",
    "num_samples = len(test_set)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for name, label in test_set:\n",
    "        hidden_state = model.init_hidden()\n",
    "        for char in name:\n",
    "            output, hidden_state = model(char, hidden_state)\n",
    "        _, pred = torch.max(output, dim=1)\n",
    "        num_correct += bool(pred == label)\n",
    "\n",
    "print(f\"Accuracy: {num_correct / num_samples * 100:.4f}%\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: tensor([[ -5.2481,  -2.1307,  -7.1686,  -1.2919,  -2.9700,  -6.0612,  -3.1607,\n",
      "          -3.5493,  -4.0597,  -0.3481,  -5.9803,  -9.0225,  -6.2125,  -8.1270,\n",
      "          -5.1937,  -3.6715,  -4.4746, -10.1209]])\n",
      "a: tensor([-0.3481])\n",
      "pred: tensor([9])\n",
      "Ethnicity of Mike is 'English'\n"
     ]
    }
   ],
   "source": [
    "label2lang = {label.item():lang for lang, label in lang2label.items()}\n",
    "def predict(name):\n",
    "    model.eval()\n",
    "    name_tensor = name2tensor(name, vocab)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        hidden_state = model.init_hidden()\n",
    "        for char in name_tensor:\n",
    "            output, hidden_state = model(char, hidden_state)\n",
    "        print(f'output: {output}')\n",
    "        a, pred = torch.max(output, 1)\n",
    "        print(f\"a: {a}\")    \n",
    "        print(f\"pred: {pred}\")\n",
    "        \n",
    "    model.train()\n",
    "    \n",
    "    return label2lang[pred.item()]\n",
    "\n",
    "\n",
    "name = 'Mike'\n",
    "print(f\"Ethnicity of {name} is '{predict(name)}'\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model state_dict\n",
    "#torch.save(model.state_dict(), 'models/classification_rnn.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## THE END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "082e9a3bcad0a290d0001e938aa60b99250c6c2ef33a923c00b70f9826caf4b7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
